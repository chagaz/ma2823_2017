{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#  2017-11-24  Tree-based methods\n",
    "\n",
    "The goal of this lab is to explore and understand tree-based models on classification problems.\n",
    "\n",
    "We will focus successively on decision trees, bagging trees and random forests. The first Bonus part guides you in implementing these algortihms yourself. The second Bonus part shows you how to use tree-based methods for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Classification data\n",
    "We will use the same data as in Lab 4: the samples are tumors, each described by the expression (= the abundance) of 3,000 genes. The goal is to separate the endometrium tumors from the uterine ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load the endometrium vs. uterus tumor data\n",
    "endometrium_data = pd.read_csv('data/small_Endometrium_Uterus.csv', sep=\",\")  # load data\n",
    "endometrium_data.head(n=5)  # adjust n to view more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the design matrix and target vector\n",
    "X = endometrium_data.drop(['ID_REF', 'Tissue'], axis=1).values\n",
    "y = pd.get_dummies(endometrium_data['Tissue']).values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## make folds\n",
    "from sklearn import model_selection\n",
    "skf = model_selection.StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X, y)\n",
    "folds = [(tr,te) for (tr,te) in skf.split(X, y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cross-validation procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_clf(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        classifier.fit(design_matrix[tr,:], labels[tr])\n",
    "        pos_idx = list(classifier.classes_).index(1)\n",
    "        pred[te] = (classifier.predict_proba(design_matrix[te,:]))[:, pos_idx]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_clf_optimize(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        classifier.fit(design_matrix[tr,:], labels[tr])\n",
    "        print(classifier.best_params_)\n",
    "        pos_idx = list(classifier.best_estimator_.classes_).index(1)\n",
    "        pred[te] = (classifier.predict_proba(design_matrix[te,:]))[:, pos_idx]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Decision Trees\n",
    "A decision tree predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "In scikit-learn, they are implemented in [tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for classification and [tree.DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.1 Toy dataset\n",
    "In order to better understand how a decision tree processes the feature space, we will first work on a simulated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-afc6304052a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "x1 = np.random.multivariate_normal([2,2], [[0.1,0],[0,0.1]], 50)\n",
    "x2 = np.random.multivariate_normal([-2,-2], [[0.1,0],[0,0.1]], 50)\n",
    "x3 = np.random.multivariate_normal([-3,3], [[0.1,0.1],[0,0.1]], 50)\n",
    "X1 = np.concatenate((x1,x2,x3), axis=0)\n",
    "\n",
    "y1 = np.random.multivariate_normal([-2,2], [[0.1,0],[0,0.1]], 50)\n",
    "y2 = np.random.multivariate_normal([2,-2], [[0.1,0],[0,0.1]], 50)\n",
    "y3 = np.random.multivariate_normal([-3,-3], [[0.01,0],[0,0.01]], 50)\n",
    "X2 = np.concatenate((y1,y2,y3), axis=0)\n",
    "\n",
    "plt.plot(X1[:,0],X1[:,1], 'x', color='blue', label='class 1')\n",
    "plt.plot(X2[:,0], X2[:,1], 'x', color='orange', label='class 2')\n",
    "\n",
    "\n",
    "plt.legend(loc=(0.4, 0.8), fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** What do you expect the decision boudaries to look like ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ Fill-in the following code to train a decision tree on this toy data and visualize it. \n",
    "\n",
    "Change the splitter to random, meaning that the algorithm will consider the feature along which to split _randomly_ (rather than picking the optimal one), and then select the best among several _random_ splitting point. Run the algorithm several times. What do you observer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_demo = np.concatenate((X1, X2), axis=0)\n",
    "y_demo = np.concatenate((np.zeros(X1.shape[0]), np.ones(X2.shape[0])))\n",
    "\n",
    "# Train a DecisionTreeClassifier on the training data\n",
    "clf = # TODO\n",
    "\n",
    "# Create a mesh, i.e. a fine grid of values between the minimum and maximum\n",
    "# values of x1 and x2 in the training data\n",
    "plot_step = 0.02\n",
    "x_min, x_max = X_demo[:, 0].min() - 1, X_demo[:, 0].max() + 1\n",
    "y_min, y_max = X_demo[:, 1].min() - 1, X_demo[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "# Label each point of the mesh with the trained DecisionTreeClassifier\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contours corresponding to these labels \n",
    "# (i.e. the decision boundary of the DecisionTreeClassifier)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the training data \n",
    "plt.plot(X1[:,0], X1[:,1], 'x', label='class 1')\n",
    "plt.plot(X2[:,0], X2[:,1], 'x', label='class 2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2 Tumor classification data\n",
    "\n",
    "Let us now go back to our tumor classification problem.\n",
    "\n",
    "**Question:** Cross-validate 5 different decision trees (with default parameters) and print out their accuracy. Why do you get different values? Check the documentation for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "ypred_dt = [] # will hold the 5 arrays of predictions (1 per tree)\n",
    "for tree_index in range(5):\n",
    "    # Initialize a DecisionTreeClassifier\n",
    "    clf = # TODO \n",
    "    \n",
    "    # Cross-validate this DecisionTreeClassifier on the toy data\n",
    "    pred_proba = cross_validate_clf(X, y, clf, folds)\n",
    "    \n",
    "    # Append the prediction to ypred_dt \n",
    "    ypred_dt.append(pred_proba)\n",
    "    \n",
    "    # Print the accuracy of DecisionTreeClassifier\n",
    "    print(\"%.3f\" % metrics.accuracy_score(y, np.where(pred_proba > 0.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** Compute the mean and standard deviation of the area under the ROC curve of these 5 trees. Plot the ROC curves of these 5 trees.\n",
    "\n",
    "Use the [metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) module of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fpr_dt = [] # will hold the 5 arrays of false positive rates (1 per tree)\n",
    "tpr_dt = [] # will hold the 5 arrays of true positive rates (1 per tree)\n",
    "auc_dt = [] # will hold the 5 areas under the ROC curve (1 per tree)\n",
    "\n",
    "for tree_index in range(5):\n",
    "    # Compute the ROC curve of the current tree\n",
    "    fpr_dt_tmp, tpr_dt_tmp, thresholds =  metrics.roc_curve(# TODO\n",
    "    # Compute the area under the ROC curve of the current tree\n",
    "    auc_dt_tmp = metrics.auc(fpr_dt_tmp, tpr_dt_tmp)\n",
    "    fpr_dt.append(fpr_dt_tmp)\n",
    "    tpr_dt.append(tpr_dt_tmp)\n",
    "    auc_dt.append(auc_dt_tmp)\n",
    "\n",
    "# Plot the first 4 ROC curves\n",
    "for tree_index in range(4):\n",
    "    plt.plot(# TODO\n",
    "            \n",
    "# Plot the last ROC curve, with a label that gives the mean/std AUC\n",
    "plt.plot(fpr_dt[-1], tpr_dt[-1], '-', \n",
    "         label='DT (AUC = %0.2f +/- %0.2f)' % (np.mean(auc_dt), np.std(auc_dt)))\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curves', fontsize=16)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** What parameters of DecisionTreeClassifier can you play with to define trees differently than with the default parameters? Cross-validate these using a grid search with [model_selection.GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Plot the optimal decision tree on the previous plot. Did you manage to improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "param_grid = # TODO\n",
    "\n",
    "# Initialize a GridSearchCV object that will be used to cross-validate\n",
    "# a DecisionTreeClassifier with these parameters.\n",
    "# What scoring function do you want to use?\n",
    "clf = model_selection.GridSearchCV( # TODO\n",
    "\n",
    "# Cross-validate the GridSearchCV object \n",
    "ypred_dt_opt = cross_validate_clf_optimize(X, y, clf, folds)\n",
    "\n",
    "# Compute the ROC curve for the optimized DecisionTreeClassifier\n",
    "fpr_dt_opt, tpr_dt_opt, thresholds = metrics.roc_curve(y, ypred_dt_opt, pos_label=1)\n",
    "auc_dt_opt = metrics.auc(fpr_dt_opt, tpr_dt_opt)\n",
    "\n",
    "# Plot the ROC curves of the 5 decision trees from earlier\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "for tree_index in range(4):\n",
    "    plt.plot(fpr_dt[tree_index], tpr_dt[tree_index], '-', color='blue') \n",
    "plt.plot(fpr_dt[-1], tpr_dt[-1], '-', color='blue', \n",
    "         label='DT (AUC = %0.2f (+/- %0.2f))' % (np.mean(auc_dt), np.std(auc_dt)))\n",
    "\n",
    "# Plot the ROC curve of the optimized DecisionTreeClassifier\n",
    "plt.plot(fpr_dt_opt, tpr_dt_opt, color='orange', label='DT optimized (AUC=%0.2f)' % auc_dt_opt)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curves', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.2 Bagging trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will resort to ensemble methods to try to improve the performance of single decision trees. Let us start with _bagging trees_: The different trees are to be built using a _bootstrap sample_ of the data, that is to say, a sample built by randomly drawing n points _with replacement_ from the original data, where n is the number of points in the training set.\n",
    "\n",
    "Bagging is efficient when used with low bias and high variance weak learners. Indeed, by averaging such estimators, we lower the variance by obtaining a smoother estimator, which is still centered around the true density (low bias). \n",
    "\n",
    "Bagging decision trees hence makes sense, as decision trees have:\n",
    "* low bias: intuitively, the conditions that are checked become multiplicative so the tree is continuously narrowing down on the data (the tree becomes highly tuned to the data present in the training set).\n",
    "* high variance: decision trees are very sensitive to where it splits and how it splits. Therefore, even small changes in input variable values might result in very different tree structure.\n",
    "\n",
    "\n",
    "**Note**: Bagging trees and random forests start being really powerful when using large number of trees (several hundreds). This is computationally more intensive, especially when the number of features is large, as in this lab. For the sake of computational time, we suggeste using small numbers of trees, but you might want to repeat this lab for larger number of trees at home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question** Cross-validate a bagging ensemble of 5 decision trees on the data. Plot the resulting ROC curve, compared to the 5 decision trees you trained earlier.\n",
    "\n",
    "Use [ensemble.BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "# Initialize a bag of trees\n",
    "clf = # TODO\n",
    "\n",
    "# Cross-validate the bagging trees on the tumor data\n",
    "ypred_bt = cross_validate_clf(X, y, clf, folds)\n",
    "\n",
    "# Compute the ROC curve of the bagging trees\n",
    "fpr_bt, tpr_bt, thresholds = metrics.roc_curve(y, ypred_bt, pos_label=1)\n",
    "auc_bt = metrics.auc(fpr_bt, tpr_bt)\n",
    "\n",
    "# Plot the ROC curve of the 5 decision trees from earlier\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "for tree_index in range(4):\n",
    "    plt.plot(fpr_dt[tree_index], tpr_dt[tree_index], '-', color='blue') \n",
    "plt.plot(fpr_dt[-1], tpr_dt[-1], '-', color='blue', \n",
    "         label='DT (AUC = %0.2f (+/- %0.2f))' % (np.mean(auc_dt), np.std(auc_dt)))\n",
    "\n",
    "# Plot the ROC curve of the bagging trees\n",
    "plt.plot(fpr_bt, tpr_bt, color='orange', label='BT (AUC=%0.2f)' % auc_bt)\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curves', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question:__ How do the bagging trees perform compared to individual trees?\n",
    "    \n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question** Use cross_validate_optimize to optimize the number of decision trees to use in the bagging method. How many trees did you find to be an optimal choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of trees to use\n",
    "list_n_trees = [5, 10, 20, 50, 80]\n",
    "\n",
    "# Start a ROC curve plot\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "    \n",
    "for idx, n_trees in enumerate(list_n_trees):\n",
    "    # Initialize a bag of trees with n_trees trees\n",
    "    clf = # TODO\n",
    "    \n",
    "    # Cross-validate the bagging trees on the tumor data\n",
    "    ypred_bt_tmp = cross_validate_clf(X, y, clf, folds)\n",
    "    \n",
    "    # Compute the ROC curve \n",
    "    fpr_bt_tmp, tpr_bt_tmp, thresholds = metrics.roc_curve(y, ypred_bt_tmp, pos_label=1)\n",
    "    auc_bt_tmp = metrics.auc(fpr_bt_tmp, tpr_bt_tmp)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.plot(fpr_bt_tmp, tpr_bt_tmp, '-', \n",
    "             label='BT %0.f trees (AUC = %0.2f)' % (n_trees, auc_bt_opt))\n",
    "\n",
    "# Plot the ROC curve of the optimal decision tree\n",
    "plt.plot(fpr_dt_opt, tpr_dt_opt, label='DT optimized (AUC=%0.2f)' % auc_dt_opt)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curves', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In practice, simply bagging is typically not enough. In order to get a good reduction in variance, we require that the models being aggregated be uncorrelated, so that they make “different errors”. Bagging will usually get you highly correlated models that will make the same errors, and will therefore not reduce the variance of the combined predictor.\n",
    "\n",
    "**Question** What is the difference between bagging trees and random forests? How does it intuitively fix the problem of correlations between trees ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question** Cross-validate a random forest of 5 decision trees on the data. Plot the resulting ROC curve, compared to the bagging tree made of 5 decision trees.\n",
    "\n",
    "Use [ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize a random forest with 5 trees\n",
    "clf = # TODO\n",
    "\n",
    "# Cross-validate the random forest on the tumor data\n",
    "ypred_rf = # TODO\n",
    "\n",
    "# Compute the ROC curve of the random forest\n",
    "fpr_rf, tpr_rf, thresholds = # TODO\n",
    "auc_rf = # TODO\n",
    "\n",
    "# Plot the ROC curve of the 5 decision trees from earlier\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "for tree_index in range(4):\n",
    "    plt.plot(fpr_dt[tree_index], tpr_dt[tree_index], '-', color='grey') \n",
    "plt.plot(fpr_dt[-1], tpr_dt[-1], '-', color='grey', \n",
    "         label='DT (AUC = %0.2f (+/- %0.2f))' % (np.mean(auc_dt), np.std(auc_dt)))\n",
    "\n",
    "# Plot the ROC curve of the bagging trees (5 trees)\n",
    "plt.plot(fpr_bt, tpr_bt, label='BT (AUC=%0.2f)' % auc_bt)\n",
    "\n",
    "# Plot the ROC curve of the random forest (5 trees)\n",
    "plt.plot(fpr_rf, tpr_rf, label='BT (AUC=%0.2f)' % auc_bt)\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curves', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question** What are the main parameters of Random Forest which can be optimized ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question** Use cross_validate_clf_optimize to optimize \n",
    "* the number of decision trees \n",
    "* the number of features to consider at each split.\n",
    "\n",
    "How many trees do you find to be an optimal choice? How does the optimal random forest compare to the optimal bagging trees? How do the training times of the random forest and the bagging trees compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the grid of parameters to test\n",
    "param_grid = # TODO\n",
    "\n",
    "# Initialize a GridSearchCV object that will be used to cross-validate\n",
    "# a random forest with these parameters.\n",
    "# What scoring function do you want to use?\n",
    "clf = grid_search.GridSearchCV(# TODO\n",
    "\n",
    "# Cross-validate the GridSearchCV object \n",
    "ypred_rf_opt = cross_validate_clf_optimize(X, y, clf, folds)\n",
    "\n",
    "# Compute the ROC curve for the optimized random forest\n",
    "fpr_rf_opt, tpr_rf_opt, thresholds = metrics.roc_curve(y, ypred_rf_opt, pos_label=1)\n",
    "auc_rf_opt = metrics.auc(fpr_rf_opt, tpr_rf_opt)\n",
    "\n",
    "# Plot the ROC curve of the optimized DecisionTreeClassifier\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(fpr_dt_opt, tpr_dt_opt, color='grey', \n",
    "         label='DT optimized (AUC=%0.2f)' % auc_dt_opt)\n",
    "    \n",
    "# Plot the ROC curve of the optimized random forest\n",
    "plt.plot(fpr_bt_opt, tpr_bt_opt, \n",
    "         label='BT optimized (AUC=%0.2f)' % auc_bt_opt)\n",
    "\n",
    "# Plot the ROC curve of the optimized bagging trees\n",
    "plt.plot(fpr_rf_opt, tpr_rf_opt, l\n",
    "         abel='RF optimized (AUC = %0.2f' % (auc_rf_opt))\n",
    "    \n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curves', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question** How do your tree-based classifiers compare to regularized logistic regression models? \n",
    "Plot the corresponding ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Evaluate an optimized l1-regularized logistic regression\n",
    "param_grid = {'C': np.logspace(-3, 3, 7)}\n",
    "clf = grid_search.GridSearchCV(linear_model.LogisticRegression(penalty='l1'), \n",
    "                               param_grid, scoring='roc_auc')\n",
    "ypred_l1 = cross_validate_clf_optimize(X, y, clf, folds)\n",
    "fpr_l1, tpr_l1, thresholds_l1 = metrics.roc_curve(y, ypred_l1, pos_label=1)\n",
    "auc_l1 = metrics.auc(fpr_l1, tpr_l1)\n",
    "print('nb features of best sparse model:', len(np.where(clf.best_estimator_.coef_!=0)[0]))\n",
    "\n",
    "# Evaluate an optimized l2-regularized logistic regression\n",
    "clf = grid_search.GridSearchCV(linear_model.LogisticRegression(penalty='l2'), \n",
    "                               param_grid, scoring='roc_auc')\n",
    "ypred_l2 = cross_validate_clf_optimize(X, y, clf, folds)\n",
    "fpr_l2, tpr_l2, thresholds_l2 = metrics.roc_curve(y, ypred_l2, pos_label=1)\n",
    "auc_l2 = metrics.auc(fpr_l2, tpr_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curves\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(fpr_rf_opt, tpr_rf_opt, \n",
    "         label='RF optimized (AUC = %0.2f)' % (auc_rf_opt))\n",
    "plt.plot(fpr_bt_opt, tpr_bt_opt, \n",
    "         label='BT optimized (AUC = %0.2f)' % (auc_bt_opt))\n",
    "plt.plot(fpr_l1, tpr_l1,  \n",
    "         label='l1 optimized (AUC = %0.2f)' % (auc_l1))\n",
    "plt.plot(fpr_l2, tpr_l2,  \n",
    "         label='l2 optimized (AUC = %0.2f)' % (auc_l2))\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curves', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. BONUS: implementation of tree-based classifiers\n",
    "Remark: The decision tree tutorial is inspired by [link](https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/).\n",
    "\n",
    "For scalability purposes, we will work on the UCI Iris dataset, which you can load directly in scikit-learn via [datasets.load_iris](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). Read the documentation to learn more about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load the iris data\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "indices = np.union1d(np.where(y_iris==1),np.where(y_iris==2))\n",
    "X_iris = X_iris[indices,:]\n",
    "y_iris = y_iris[indices]\n",
    "y_iris[np.where(y_iris==2)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split the data into 5 folds\n",
    "from sklearn import model_selection\n",
    "kf = model_selection.KFold(n_splits=5)\n",
    "kf.get_n_splits(X_iris, y_iris)\n",
    "folds_iris = [(tr,te) for (tr,te) in skf.split(X_iris, y_iris)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.1. Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For initializing the `DecisionTreeClassifier` class, we need to decide when to stop growing a tree.\n",
    "\n",
    "We can do that using the depth and the number of rows that the node is responsible for in the training dataset.\n",
    "\n",
    "* **Maximum Tree Depth**. This is the maximum number of nodes from the root node of the tree. Once a maximum depth of the tree is met, we must stop splitting adding new nodes. Deeper trees are more complex and are more likely to overfit the training data.\n",
    "* **Minimum Node Records**. This is the minimum number of training patterns that a given node is responsible for. Once at or below this minimum, we must stop splitting and adding new nodes. Nodes that account for too few training patterns are expected to be too specific and are likely to overfit the training data.\n",
    "\n",
    "These two approaches will be user-specified arguments to our tree building procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    class_values : tuple, labels distinguishing classes\n",
    "    min_node_size : int, expected number of instance into leaves\n",
    "        if None, the number of instance in nodes is not a criterion for stopping growing the tree\n",
    "    max_depth : int, expected depth of the tree \n",
    "        if None, the depth of the tree is not a criterion for stopping growing the tree\n",
    "        \n",
    "    attributes:\n",
    "    -----------\n",
    "    _tree : dict, stores the tree structure\n",
    "        tree is composed by nodes (including one top node, called the root) caraterized by \n",
    "            'index': index of the variables of the split\n",
    "            'value': value of the variables of the split\n",
    "            'left': indices of instances falling into the left child node\n",
    "            'right': indices of instances falling into the right child node\n",
    "    \"\"\"\n",
    "    def __init__(self, class_values=(0,1), min_node_size=2, max_depth=None):\n",
    "        self.classes_ = class_values\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_depth = max_depth\n",
    "        self._tree = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1.1 Create a split\n",
    "\n",
    "Creating a split involves 2 parts:\n",
    "\n",
    "* Splitting a Dataset: means separating a dataset into two lists of instances given the index of a feature and a split value for that feature. \n",
    "\n",
    "* Evaluating All Splits: Once we have the two groups, we can then use the Gini score to evaluate the cost of the split. A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split. Given a dataset, we must check every value on each attribute as a candidate split, evaluate the cost of the split and find the best possible split we could make. Once the best split is found, we can use it as a node in our decision tree. This is an exhaustive and greedy algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Gini criterion\n",
    "\n",
    "For every resulting nodes of the split:\n",
    "\n",
    "`gini_index = [1.0 - sum_over_classes(proportion * proportion)] * (group_size/total_samples)`\n",
    "\n",
    "where `proportion = count(class_value) / count(instances in the node)`\n",
    "\n",
    "The gini indices are then added across each child node at the split point to give a final Gini score for the split point that can be compared to other candidate split points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(self, labels, index_groups):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels: array, \n",
    "        indexed labels \n",
    "    index_groups: list,\n",
    "        list of list of indices, each list corresponds to one group of instances indexed by their list\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    gini: float,\n",
    "        gini score of the current split\n",
    "    \"\"\"\n",
    "    n_samples = float(sum([len(group) for group in index_groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in index_groups:\n",
    "        size = float(len(group))\n",
    "        # avoid dividing by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in self.classes_:\n",
    "            score += # TODO\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size/n_samples)\n",
    "    return gini\n",
    "\n",
    "# Make gini_index a method of the DecisionTreeClassifier class\n",
    "DecisionTreeClassifier._gini_index = gini_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Splitting a Dataset\n",
    "\n",
    "Splitting a dataset means separating a dataset into two lists of rows given the index of an attribute and a split value for that attribute.\n",
    "\n",
    "Below is a function named test_split() that implements this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(self, value_feature, X_column, indices):\n",
    "    \"\"\"\n",
    "    Split a list of instances into 2 groups depending on their value for a specific feature.\n",
    "    parameters:\n",
    "    -----------\n",
    "    value_feature: float\n",
    "        split value\n",
    "    X_column: 1D array,\n",
    "        column of the design matrix corresponding to the current feature\n",
    "    indices: 1D array\n",
    "        list of indices to split\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    left, rigt: list of int\n",
    "        list of indices of instances \n",
    "    \"\"\"\n",
    "    left, right = [], []\n",
    "    for ind_instance in indices:\n",
    "        # Update left and right based on the value of the attribute of instance ind_instance\n",
    "        # TODO \n",
    "    return left, right\n",
    "\n",
    "# Make test_split a method of the DecisionTreeClassifier class\n",
    "DecisionTreeClassifier._test_split = test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Evaluating All Splits **\n",
    "\n",
    "We will use a dictionary to represent a node in the decision tree as we can store data by name. When selecting the best split and using it as a new node for the tree we will store the index of the chosen attribute, the value of that attribute by which to split and the two groups of data split by the chosen split point.\n",
    "\n",
    "Each group of data is its own small dataset of just those rows assigned to the left or right group by the splitting process. You can imagine how we might split each group again, recursively as we build out our decision tree.\n",
    "\n",
    "Below is a function `get_split()` that implements this procedure. You can see that it iterates over each features and then each value for that attribute, splitting and evaluating splits as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select the best split point for a dataset\n",
    "def get_split(self, X, y, indices):\n",
    "    \"\"\"\n",
    "    Split a list of instances into 2 groups depending on their value of a specific feature.\n",
    "    parameters:\n",
    "    -----------\n",
    "    X: array\n",
    "        design matrix\n",
    "    y: 1D array\n",
    "        labels\n",
    "    indices: 1D array\n",
    "        list of indices of instances to split\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    node: dict,\n",
    "        'index' contains the index of the feature used to split the current node, \n",
    "        'value' contains the value of the feature used to split the current node,\n",
    "        'groups' contains the list of instances of the 2 child nodes of the current node.\n",
    "    \"\"\"\n",
    "    # Best index, value, score, and groups\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    # Test each feature\n",
    "    for ind_feature in range(X.shape[1]):\n",
    "        # Test each value of the feature for the considered instances\n",
    "        for ind_instance in indices:\n",
    "            # Split the dataset (restricted to the considered instances)\n",
    "            groups = # TODO\n",
    "            # Compute the corresponding Gini Index\n",
    "            gini = # TODO\n",
    "            if gini < b_score:\n",
    "                # Update b_index, b_value, b_score, b_groups\n",
    "                # TODO \n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Make get_split a method of the DecisionTreeClassifier class\n",
    "DecisionTreeClassifier._get_split = get_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1.2 Building a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Leaves nodes are used to make a final prediction.\n",
    "\n",
    "This is done by taking the group of instances assigned to that leaf and selecting the most common class value in the group. This will be used to make predictions.\n",
    "\n",
    "Below is a function named `leaf_value` that will select a class value for a group of rows. It returns the most common output value in a list of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a leaf node value\n",
    "def leaf_value(self, labels):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels: list of int,\n",
    "        list of instances' labels in the current leaf node\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    leaf_value: tuple,\n",
    "        proportion of class 0 and class 1 instances in the current leaf node\n",
    "    \"\"\"\n",
    "    counter = Counter(labels.tolist())\n",
    "    return np.array([counter[0], counter[1]])\n",
    "\n",
    "# Make leaf_value a method of the DecisionTreeClassifier class\n",
    "DecisionTreeClassifier._leaf_value = leaf_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once a node is created, we can create child nodes recursively on each group of data from the split by calling the same function again.\n",
    "\n",
    "Below is a function that implements this recursive procedure. It takes a node as an argument as well as the maximum depth, minimum number of patterns in a node and the current depth of a node.\n",
    "\n",
    "Recall, there are three reasons to stop growing the tree:\n",
    "    either left or right node is empty and if so we create a leaf node using what records we do have.\n",
    "    the current depth reached the maximum depth and if so we create a leaf node.\n",
    "    the size of the node reached the minimum size, if so we create a leaf node\n",
    "\n",
    "If none of these situations occur, we keep on growing the tree by splitting the branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split(self, X, y, node, depth):\n",
    "    \"\"\"\n",
    "    Create the tree structure by reccursively splitting\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array,\n",
    "        design matrix\n",
    "    y: 1D array,\n",
    "        labels\n",
    "    node: dict,\n",
    "        current node\n",
    "    depth: int,\n",
    "        depth of the current node\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    either call _leaf_value or _get_split functions \n",
    "    \"\"\"\n",
    "    left, right = node['groups']\n",
    "    # As we work on these groups the node no longer requires access to these data\n",
    "    del(node['groups'])\n",
    "    # check for a no split, i.e.  we check if either left or right group of rows is empty \n",
    "    # and if so we create a terminal node using what records we do have.\n",
    "    if not left or not right:\n",
    "        # Update left and right branches\n",
    "        node['left'] = node['right'] = self._leaf_value(y[left + right])\n",
    "        return\n",
    "    # check for max depth\n",
    "    if self.max_depth is not None and depth >= self.max_depth:\n",
    "        # Update left and right branches\n",
    "        node['left'], node['right'] = # TODO\n",
    "        return\n",
    "    # process left child\n",
    "    if self.min_node_size is not None and len(left) <= self.min_node_size:\n",
    "        node['left'] = self._leaf_value(y[left])\n",
    "    else:\n",
    "        node['left'] = self._get_split(X, y, left)\n",
    "        self._split(X, y, node['left'], depth+1)\n",
    "    # process right child\n",
    "    # TO DO\n",
    "\n",
    "# Make split a method of the DecisionTreeClassifier class\n",
    "DecisionTreeClassifier._split = split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Building the tree involves creating the root node and calling the `split` function that then calls itself recursively to build out the whole tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fit(self, X_tr, y_tr):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_tr: array,\n",
    "        training design matrix\n",
    "    y_tr: 1D array,\n",
    "        training labels\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    tree: tree structure (dict),\n",
    "    \"\"\"\n",
    "    # Get the first node (root)\n",
    "    self._Tree = self._get_split(X_tr, y_tr, np.arange(X_tr.shape[0]))\n",
    "    \n",
    "    # Get the tree structure\n",
    "    # TODO\n",
    "\n",
    "    return self._Tree\n",
    "\n",
    "# Make fit a method of the DecisionTreeClassifier class\n",
    "DecisionTreeClassifier.fit = fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1.3 Make a Prediction\n",
    "\n",
    "Making predictions with a decision tree involves navigating the tree with the specifically provided row of data.\n",
    "\n",
    "Again, we can implement this using a recursive function, where the same prediction routine is called again with the left or the right child nodes, depending on how the split affects the provided data.\n",
    "\n",
    "We must check if a child node is either a terminal value to be returned as the prediction, or if it is a dictionary node containing another level of the tree to be considered.\n",
    "\n",
    "Below is the `passing_tree` function that implements this procedure. The `predict` function wraps it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def passing_tree(self, node, X_row):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    node: dict,\n",
    "        current node\n",
    "    X_row: 1d array,\n",
    "        features array for the current instance\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    leaf_value: tuple,\n",
    "        proportion of class 0 and class 1 instances in the leaf node in which the current instance falls into.\n",
    "    \"\"\"\n",
    "    if X_row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return self._passing_tree(node['left'], X_row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return self._passing_tree(node['right'], X_row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "# Make passing_tree a method of the DecisionTreeClassifier class\n",
    "DecisionTreeClassifier._passing_tree = passing_tree\n",
    "\n",
    "def predict(self, X_te):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_te: array,\n",
    "        test design matrix\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    predicted_labels: list,\n",
    "        list of predicted labels for each instances in the test set\n",
    "    \"\"\"\n",
    "    predictions = np.zeros(X_te.shape[0])\n",
    "    i = 0\n",
    "    for row in X_te:\n",
    "        prediction[i] = # TODO\n",
    "        i += 1\n",
    "    return(predictions)\n",
    "\n",
    "# Make predict a method of the DecisionTreeClassifier class\n",
    "DecisionTreeClassifier.predict = predict\n",
    "\n",
    "def predict_proba(self, X_te):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_te: array,\n",
    "        test design matrix\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    predicted_probability_labels: list,\n",
    "        list of predicted labels probability for each instances in the test set\n",
    "    \"\"\"\n",
    "    predictions = np.zeros((X_te.shape[0],2))\n",
    "    i=0\n",
    "    for row in X_te:\n",
    "        predictions[i, :] = # TODO\n",
    "        i += 1\n",
    "    return(predictions)\n",
    "\n",
    "# Make predict_proba a method of the DecisionTreeClassifier class\n",
    "DecisionTreeClassifier.predict_proba = predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question** Cross-validate the decision tree and display the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "pred = cross_validate_clf(design_matrix=X_iris, \n",
    "                          classifier=clf, cv_folds=folds_iris, labels=y_iris)\n",
    "print(metrics.accuracy_score(y_iris, np.where(pred > 0.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1.4 Possible extensions\n",
    "\n",
    "**Cross Entropy.** Another cost function for evaluating splits quality is cross entropy (logloss). \n",
    "\n",
    "**Categorical Dataset.** Our `DecisionTreeClassifier` class was designed for input data with numerical or ordinal input attributes, experiment with categorical input data and splits that may use equality instead of ranking.\n",
    "\n",
    "**Regression.** We can adapt the tree for regression using a different cost function (like the mean squared error) and method for creating leaves (like the mean of the values associated to the instances of the corresponding leaf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.2 Bagging trees\n",
    "\n",
    "Fill in the blanks to create our own BaggingTreesClassifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_tree(self, X_tr, y_tr):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_tr: array,\n",
    "        training design matrix\n",
    "    y_tr: 1d array,\n",
    "        training labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tree: tree structure (dict),\n",
    "        the tree structure learned on the bootstrapped data.\n",
    "    \"\"\"\n",
    "    # Create a bootstrap sample\n",
    "    boot = np.random.choice(np.arange(X_tr.shape[0]), size=X_tr.shape[0], replace=True)\n",
    "    # Train a tree on this bootstrap sample\n",
    "    local_tree = # TODO\n",
    "    return local_tree\n",
    "\n",
    "def get_pred(self, X_te, local_tree, pred_type):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_te: array,\n",
    "        test design matrix\n",
    "    pred_type: string,\n",
    "        either 'label' (for returnin predicted labels) or 'proba' (for returning labels probability)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    trees_prediction: list,\n",
    "        list of predictions\n",
    "    \"\"\"\n",
    "    if pred_type=='label':\n",
    "        predictions = np.zeros(X_te.shape[0])\n",
    "        i = 0\n",
    "        for row in X_te:\n",
    "            predictions[i] = # TODO\n",
    "            i += 1\n",
    "    elif pred_type=='proba':\n",
    "        predictions = np.zeros((X_te.shape[0], 2))\n",
    "        i=0\n",
    "        for row in X_te:\n",
    "            predictions[i, :] = # TODO\n",
    "            i += 1\n",
    "    return predictions\n",
    "\n",
    "def get_trees_prediction(self, X_te, pred_type):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_te: array,\n",
    "        test design matrix\n",
    "    pred_type: string,\n",
    "        either 'label' (for returnin predicted labels) or 'proba' (for returning labels probability)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    trees_prediction: list,\n",
    "        list of predictions\n",
    "    \"\"\"\n",
    "    trees_prediction = []\n",
    "    for n in range(self.n_trees):\n",
    "        trees_prediction.append( # TODO\n",
    "    return trees_prediction\n",
    "\n",
    "        \n",
    "class BaggingTreesClassifier(DecisionTreeClassifier):\n",
    "    def __init__(self, class_values=[0,1], min_size=1, max_depth=None, n_trees=5):\n",
    "        DecisionTreeClassifier.__init__(self, class_values, min_size, max_depth)\n",
    "        self.n_trees = n_trees\n",
    "        self._trees = []\n",
    "        \n",
    "    def fit(self, X_tr, y_tr):\n",
    "        self.n_features = X_tr.shape[1]\n",
    "        for n in range(self.n_trees):\n",
    "            self._trees.append(get_tree(self,X_tr,y_tr))\n",
    "    \n",
    "    def predict(self, X_te):\n",
    "        trees_prediction = get_trees_prediction(self, X_te, 'label')\n",
    "        return np.where(np.sum(trees_prediction, axis=0) > self.n_trees, 1, 0)\n",
    "    \n",
    "    def predict_proba(self,X_te):\n",
    "        trees_prediction = get_trees_prediction(self, X_te, 'proba')\n",
    "        return np.sum(trees_prediction, axis=0)/self.n_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Bonus question ** Add a parallelized implementation, using the [multiprocessing.Pool](https://docs.python.org/3.5/library/multiprocessing.html#module-multiprocessing) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def get_trees_prediction_parallel(self, X_te, pred_type):\n",
    "    if type(self.parallel) is not int:\n",
    "        trees_prediction = []\n",
    "        for n in range(self.n_trees):\n",
    "            trees_prediction.append(get_pred(self, X_te, self._trees[n], pred_type))\n",
    "    else:\n",
    "        n_cpu = self.parallel\n",
    "        p = Pool(n_cpu)\n",
    "        trees_prediction = p.starmap(# TODO\n",
    "    return trees_prediction\n",
    "\n",
    "class BaggingTreesClassifier_parallel(DecisionTreeClassifier):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    class_values: tuple\n",
    "        labels distinguishing classes\n",
    "    min_size: int\n",
    "        expected number of instance into leaves\n",
    "        if None, the number of instance in nodes is not a criterion for stopping growing the tree\n",
    "    max_depth: int\n",
    "        expected depth of the tree \n",
    "        if None, the depth of the tree is not a criterion for stopping growing the tree\n",
    "    n_trees: int\n",
    "        number of trees\n",
    "    parallel: int\n",
    "        number of CPU cores to use\n",
    "        if None, use sequential implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_values=[0,1], min_size=1, max_depth=None, n_trees=5, parallel=None):\n",
    "        DecisionTreeClassifier.__init__(self, class_values, min_size, max_depth)\n",
    "        self.n_trees = n_trees\n",
    "        self._trees = []\n",
    "        self.parallel = parallel\n",
    "        \n",
    "    def fit(self, X_tr, y_tr):\n",
    "        n_samples, self.n_features = X_tr.shape[0], X_tr.shape[1]\n",
    "        if type(self.parallel) is not int:\n",
    "            # Compute trees sequentially\n",
    "            for n in range(self.n_trees):\n",
    "                self._trees.append(get_tree(self, n_samples, X_tr, y_tr))\n",
    "        else:\n",
    "            # Compute trees in parallel\n",
    "            n_cpu = self.parallel\n",
    "            p = Pool(n_cpu)\n",
    "            self._trees = p.starmap(# TODO\n",
    "    \n",
    "    def predict(self, X_te):\n",
    "        trees_prediction = get_trees_prediction_parallel(self, X_te, 'label')\n",
    "        return np.where(np.sum(trees_prediction, axis=0) > self.n_trees, 1, 0)\n",
    "    \n",
    "    def predict_proba(self,X_te):\n",
    "        trees_prediction = get_trees_prediction_parallel(self, X_te, 'proba')\n",
    "        return np.sum(trees_prediction, axis=0)/self.n_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question** Compute the predicted labels by a bagging trees classifier of 5 decision trees on the test data. Display the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = BaggingTreesClassifier()\n",
    "pred = cross_validate_clf(design_matrix=X_iris, classifier=clf, cv_folds=folds_iris, labels=y_iris)\n",
    "print(metrics.accuracy_score(y_iris, np.where(pred > 0.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question** Implement a new class called RandomForest inheriting from the class BaggingTreesClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RandomForestClassifier(BaggingTreesClassifier):\n",
    "    def __init__(self, class_values=[0,1], min_size=1, max_depth=None, n_trees=10, max_features='auto'):\n",
    "        BaggingTreesClassifier.__init__(self, class_values, min_size, max_depth, n_trees)\n",
    "        self.max_features = max_features\n",
    "        # Number of features to consider per split:\n",
    "        self._features_per_split = None if type(max_features) is not int else max_features\n",
    "    \n",
    "    def fit(self, X_tr, y_tr):\n",
    "        if self.max_features == 'auto':\n",
    "            # As in sklearn, use the square root of the number of features\n",
    "            self._features_per_split = # TODO\n",
    "        # TODO train the random forest classifier\n",
    "        \n",
    "    # Select the best split point for a dataset\n",
    "    def _get_split(self, X, y, node_indices):\n",
    "        b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "        features = np.random.choice(np.arange(self.n_features), size=self._features_per_split, replace=False)\n",
    "        for ind_feature in features:\n",
    "            for ind_instance in node_indices:\n",
    "                # Split the dataset (restricted to the considered instances)\n",
    "                groups = # TODO\n",
    "                # Compute the corresponding Gini Index\n",
    "                gini = # TODO\n",
    "                if gini < b_score:\n",
    "                    # Update b_index, b\n",
    "        return {'index':b_index, 'value':b_value, 'groups':b_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_features=1)\n",
    "pred = cross_validate_clf(design_matrix=X_iris, classifier=clf, cv_folds=folds_iris, labels=y_iris)\n",
    "print(metrics.accuracy_score(y_iris, np.where(pred > 0.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. BONUS: feature importance via RandomForest\n",
    "\n",
    "Based on the performance obtained by the l1-regularized logistic regression, on the endometrium-vs-uterus dataset, a subset of features can yield better predictive models of gene expression data. \n",
    "\n",
    "It is worth to notice that tree-based methods 'naturally' compute a measure of feature importance which can be directly use for selecting features. Indeed, at each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splittings variables. Feature importance is accumulated over all trees in the forest.\n",
    "\n",
    "In scikit-learn, this feature importance is accessible via the `feature_importances_` attributes of random forests, and can be processed thanks to the meta-transformer [feature_selection.SelectFromModel](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading modules \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import pipeline\n",
    "\n",
    "#from sklearn import ensemble\n",
    "#from sklearn import linear_model\n",
    "#from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reloading the data\n",
    "cancer_data = pd.read_csv('data/small_Endometrium_Uterus.csv', sep=\",\")  # load data\n",
    "X = cancer_data.drop(['ID_REF', 'Tissue'], axis=1).values\n",
    "y = pd.get_dummies(cancer_data['Tissue']).values[:,1]\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating folds\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X, y)\n",
    "folds = [(tr,te) for (tr,te) in skf.split(X, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD_OPTIONS = ['mean', '1.5*mean', '2*mean', '5*mean']\n",
    "C_OPTIONS = np.logspace(-3, 3, 7)\n",
    "N_ESTIMATORS_OPTIONS = [10, 20, 50]\n",
    "\n",
    "param_grid = [\n",
    "\n",
    "    {\n",
    "        'feature_selection': [SelectFromModel(ensemble.RandomForestClassifier(n_estimators=50))],\n",
    "        'feature_selection__threshold': THRESHOLD_OPTIONS,\n",
    "        'classification': [ensemble.RandomForestClassifier()],\n",
    "        'classification__n_estimators': N_ESTIMATORS_OPTIONS,\n",
    "    },\n",
    "    {\n",
    "        'feature_selection': [SelectFromModel(ensemble.RandomForestClassifier(n_estimators=50))],\n",
    "        'feature_selection__threshold': THRESHOLD_OPTIONS,\n",
    "        'classification': [linear_model.LogisticRegression(penalty='l2')],\n",
    "        'classification__C': C_OPTIONS,\n",
    "    },\n",
    "]\n",
    "\n",
    "pipe = pipeline.Pipeline([\n",
    "  ('feature_selection', SelectFromModel((ensemble.RandomForestClassifier(n_estimators=50)))),\n",
    "  ('classification', ensemble.RandomForestClassifier())\n",
    "])\n",
    "grid = GridSearchCV(pipe, cv=folds, n_jobs=3, param_grid=param_grid, scoring='roc_auc')\n",
    "grid.fit(X, y)\n",
    "\n",
    "grid_l1_log_reg = GridSearchCV(linear_model.LogisticRegression(penalty='l1'), cv=folds, n_jobs=3, \n",
    "                               param_grid={'C':C_OPTIONS}, scoring='roc_auc')\n",
    "grid_l1_log_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us now use a data_frame to display the results of our evaluation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_frame = {name:[] for i in range(len(grid.cv_results_['params'])) for name in grid.cv_results_['params'][i]}\n",
    "data_frame['score'] = []\n",
    "\n",
    "sorted_index_score = np.argsort(grid.cv_results_['mean_test_score'])[::-1]\n",
    "for ind in sorted_index_score:\n",
    "    data_frame['score'].append(grid.cv_results_['mean_test_score'][ind])\n",
    "    for name in data_frame.keys():\n",
    "        if name in grid.cv_results_['params'][ind]:\n",
    "            data_frame[name].append(grid.cv_results_['params'][ind][name])\n",
    "        elif name!='score':\n",
    "            data_frame[name].append(None)\n",
    "    \n",
    "pd.DataFrame(data_frame).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_frame = {name:[] for i in range(len(grid_l1_log_reg.cv_results_['params'])) \n",
    "                          for name in grid_l1_log_reg.cv_results_['params'][i]}\n",
    "data_frame['score'] = []\n",
    "\n",
    "sorted_index_score = np.argsort(grid_l1_log_reg.cv_results_['mean_test_score'])[::-1]\n",
    "for ind in sorted_index_score:\n",
    "    data_frame['score'].append(grid_l1_log_reg.cv_results_['mean_test_score'][ind])\n",
    "    for name in data_frame.keys():\n",
    "        if name in grid_l1_log_reg.cv_results_['params'][ind]:\n",
    "            data_frame[name].append(grid_l1_log_reg.cv_results_['params'][ind][name])\n",
    "        elif name!='score':\n",
    "            data_frame[name].append(None)\n",
    "    \n",
    "pd.DataFrame(data_frame).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('number of original features: ', X.shape[1])\n",
    "\n",
    "tree_based_feature_selection = SelectFromModel(estimator=ensemble.RandomForestClassifier(n_estimators=50), \n",
    "                                               threshold='mean')\n",
    "tree_based_feature_selection.fit(X, y)\n",
    "print('number of selected features by random forest:', len(tree_based_feature_selection.get_support(indices=True)))\n",
    "\n",
    "l1_clf = linear_model.LogisticRegression(penalty='l1', C=100)\n",
    "l1_clf.fit(X,y)\n",
    "print('number of selected features by log. reg. with L1 regularization:', len(np.where(l1_clf.coef_!=0)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
