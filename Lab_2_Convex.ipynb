{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-10-06  Convex optimisation\n",
    "\n",
    "The goal of this lab is to manipulate some of the convex optimisation tools from [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html), which you will need to implement the machine learning algorithms we will see in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Image, set_matplotlib_formats \n",
    "set_matplotlib_formats('pdf', 'svg') # toggle vector graphics for a crisp plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convex functions have the useful property that local minima are global minima.\n",
    "![](./img/convexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for scipy, we start with a simple example. We begin by defining a function to optimise, $$f_1(x) = (x - 1)^4 + x^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify objective function\n",
    "f1 = lambda x : (x - 1) ** 4 + x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Brent's method for optimisation. Brent's method does not require the function to optimize to be convex or derivable everywhere, and works as a combination of the secant method and parabola fittings, as follows:\n",
    "1. Take a, b such that f'(a) < 0 and f'(b) > 0\n",
    "2. Repeat:\n",
    "  * Compute c = (a+b)/2\n",
    "  * Compute the point d where the parabola that goes through a, b and c is minimal\n",
    "  * If f'(d) < 10^{-sthg}: stop\n",
    "  * Otherwise if f'(d) < 0: replace a with d, otherwise: replace b with d.\n",
    "  \n",
    "<img src=\"./img/brent.png\", width=600>\n",
    "\n",
    "Illustration's source: Press, W. H., et al. (1992) \"Numerical Recipes in Fortran, The Art of Scientific Computing\", Cambridge University Press, Chapter 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "res = minimize_scalar(f1, method='brent')\n",
    "print('xmin: %.02f, fval: %.02f, iter: %d' % (res.x, res.fun, res.nit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Briefly describe the function output.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot curve\n",
    "x = np.linspace(res.x - 0.5, res.x + 0.5, 100)\n",
    "y = [f1(val) for val in x]\n",
    "plt.plot(x, y, color='blue', label='f1')\n",
    "\n",
    "# plot optima\n",
    "plt.scatter(res.x, res.fun, color='orange', marker='x', label='opt')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.** Optimisation, and particularly convex optimisation, plays a central role in machine learning. Although in this lab we focus on optimising simple functions, the same techniques will be used to optimise the high-dimensional loss functions of our learning machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will be doing a lot of plotting, so we'll create a reusable function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_iterations(f, iterations, delta=0.1, box=1):\n",
    "    xs = np.arange(-box, +box, delta)\n",
    "    ys = np.arange(-box, +box, delta)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    # create function mesh\n",
    "    Z = np.array([[f(np.array([x, y])) for x in xs] for y in ys])\n",
    "    # contour plot\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    cd = ax.contour(X, Y, Z)\n",
    "    # error plot\n",
    "    ax.clabel(cd, inline=1, fontsize=10)\n",
    "    ax.plot(iterations[:, 0], iterations[:, 1], color='red', marker='x', label='opt')\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(range(len(iterations)), [f(np.array(step)) for step in iterations], 'xb-')\n",
    "    ax.grid('on')\n",
    "    ax.set_xlabel('iteration')\n",
    "    ax.set_xticks(range(len(iterations)))\n",
    "    ax.set_ylabel('f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ What does `plot_iterations` do?\n",
    "\n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a callback function between iterations. This function will be called after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify callback function\n",
    "def get_callback_function(data_list):\n",
    "    def callback_function(x):\n",
    "        data_list.append(x)\n",
    "    return callback_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Smooth functions\n",
    "\n",
    "When our cost function is smooth, we can use the tools of calculus for an open or closed-formed solution. We here focus on the former case, where our function is optimised incrementally in an iterative procedure. In general, we iterate until the gradient of our function is close to zero. We again start by specifing a function to minimise,\n",
    "\n",
    "\\begin{align}\n",
    "f_2(\\mathbf{x}) &= 2x_1^2 + 5x_2^2 \\\\ \\notag\n",
    "&= \\mathbf{x}^T\\mathbf{D}\\mathbf{x} \\notag\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{D} = \\begin{bmatrix}\n",
    "    2 & 0 \\\\\n",
    "    0 & 5 \\end{bmatrix}$ and $\\mathbf{x} \\in \\mathbb{R}^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2 = lambda x : x.T.dot(np.diag([2, 5])).dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute force search\n",
    "\n",
    "First we'll try a trivial optimisation algorithm: [brute force](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brute.html#scipy.optimize.brute). A brute force search tries every point in a fixed grid and retains the minimising value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import brute\n",
    "print(brute(f2, ranges=((-2, 2), (-2, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Comment briefly on the output.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First and second-order characterizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A twice-differentiable convex function has a positive semi-definite Hessian $\\mathbf{x} \\mapsto \\nabla^2 f(\\mathbf{x})$ and is minimized where the gradient $\\mathbf{x} \\mapsto \\nabla f(\\mathbf{x})$ is equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Write a function `df2()` for the Jacobian (vector of partial derivatives) of `f2()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2(x):\n",
    "    # TODO: return vector of partial derivatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Write a function `ddf2()` for the Hessian (matrix of second partial derivatives) of `f(2)` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ddf2(x):\n",
    "    # TODO: return Hessian matrix of second partial derivatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Prove our Hessian matrix, $\\mathbf{H} = \\partial^2 f / \\partial\\mathbf{x}^2$, is positive-definite everywhere.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent methods\n",
    "\n",
    "The workhorse of machine learning is gradient descent. Its variants are used to optimise a panoply of models from linear regression to deep neural nets. The gradient of mulivariate function at a point is the vector normal to the level curve. Gradient descent works by taking steps in the direction of the gradient,\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} -\\alpha\\nabla f(\\mathbf{x_k})$$\n",
    "\n",
    "![Gradient descent on a 2D convex function](img/gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's Method\n",
    "\n",
    "Newton's method is a second order method, minimising a quadratic Taylor approximation to the objective function at each point. It thus combines the gradient and Hessian matrix,\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} -\\alpha\\mathbf{H}_k^{-1}\\mathbf{g}_k$$\n",
    "\n",
    "where $\\mathbf{g}_k = \\nabla f(\\mathbf{x_k})$ and $\\mathbf{H}_k = \\nabla^2 f(\\mathbf{x_k})$. This is a multi-dimensional generalisation of the Newton root-finding method (here we are finding the root of the gradient). Newton's method typically involves a line search to optimise the size of the descent step, $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Fill in the missing lines in the cell below to run [Newton's method](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_ncg.html) on our function. Note, in this implementation, the (sparse) Hessian is inverted by conjugate gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "# TODO \n",
    "\n",
    "# specify initial point\n",
    "x0 = [-1, 1]\n",
    "# initialise callback data\n",
    "ncg_data = [np.array(x0)]\n",
    "\n",
    "# use fmin_ncg\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iterations(f2, np.array(ncg_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Newton's method uses a line search to optimise each descent step by choosing a step size $\\alpha$ that optimises the descent direction, that is, $$\\min_\\alpha f(\\mathbf{x_k} + \\alpha \\mathbf{d}_k),$$ where $\\mathbf{d}_k = \\nabla f({\\mathbf{x}_k})$ is the descent direction. How does this show in the visualisation?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quasi-Newton: BFGS\n",
    "\n",
    "[Broyden–Fletcher–Goldfarb–Shanno (BFGS)](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) is a quasi-Newton method. Because the Hessian is $\\mathcal{O}(n^2)$ (a large expense at each iteration), BFGS aims to approximate the Hessian \"on the fly\" as a sum of rank one approximations. Note when space *and* runtime are a concern, *limited-memory* BFGS (l-BFGS) is further an option.\n",
    "\n",
    "**Question:** Run the [scipy BFGS](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html#scipy.optimize.fmin_bfgs) algorithm on our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise callback data\n",
    "bfgs_data = # TODO\n",
    "\n",
    "# TODO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iterations(f2, np.array(bfgs_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Verify that the norm of the gradient at the final point is close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO calculate the norm of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do you observe about the steps?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjugate gradient method\n",
    "\n",
    "The [conjugate gradient method](https://en.wikipedia.org/wiki/Conjugate_gradient_method) is an alternative to gradient descent. For a linear system $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$, this algorithm finds a solution as a linear combination of a set of *mutually conjugate vectors*, $\\mathbf{p}_d$, such that,\n",
    "\n",
    "$$\\mathbf{x}^* = \\sum_{d=1}^D \\alpha_d\\mathbf{p}_d$$ for the $D$ dimensions of the problem. The vectors are built determined one by one in a process similar to the [Gram Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process). Particularly for a sparse system, a good approximate solution can be determined without constructing the entire conjugate set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Run the conjugate gradient method `fmin_cg` on our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise callback data\n",
    "cg_data = # TODO\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iterations(f2, np.array(cg_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Try changing the initial point. Does anything change? Why?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-smooth functions\n",
    "\n",
    "Convex functions remain convex when we add linear constraints. A linear constraint restricts the solution space with an intersecting hyperplane. However, the smoothness property is lost, and calculus is no longer an option. To conceptualise the effect, imagine taking a spherical fruit such as an apple to be our unconstrained convex function. Then, adding a linear constraint corresponds to slicing it at some position at a fixed angle. Intuitively, the fruit remains convex after the cut, despite losing its smoothness (roundness)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quadratic programming\n",
    "\n",
    "A [quadratic programming (QP) problem](https://en.wikipedia.org/wiki/Quadratic_programming) is an optimisation problem of a quadratic function $\\frac{1}{2}\\mathbf{x^TQx} + \\mathbf{c^Tx}$ subject to linear constraints, $\\mathbf{Ax} \\leq \\mathbf{b}$. QP optimisation algorithms can be applied to the training of support vector machines (SVMs). Let's optimise the following quadratic program:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\text{minimise} & z = x_1^2 + 4x_2^2 - 32x_2 + 64 \\\\\n",
    "\\text{subject to} & x_1 + x_2 \\leq 7 \\\\\n",
    "& -x_1 + 2x_2 \\leq 4 \\\\\n",
    "& x_2 \\leq 4 \\\\\n",
    "& x_1, x_2 \\geq 0\n",
    "\\end{array},\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Specify the constraint matrices, $\\mathbf{A}$ (coefficients), and $\\mathbf{b}$ (constants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the matrix of coefficients\n",
    "A = #TODO \n",
    "\n",
    "# specify the vector of constants\n",
    "b = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create dictionary of constraints for use in the optimisation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cons = {'type':'ineq',\n",
    "        'fun':lambda x: b - np.dot(A,x),\n",
    "        'jac':lambda x: -A}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Next, specify the components of the objective function: the matrix of quadratic coefficients $\\mathbf{Q}$, the vector of linear coefficients $\\mathbf{c}$, and the constant term, $c_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the matrix of quadratic coefficients\n",
    "Q = # TODO\n",
    "\n",
    "# specify the vector of linear coefficients\n",
    "c = # TODO \n",
    "\n",
    "# specify the constant\n",
    "c0 = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Write the objective function (loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    # TODO: return loss (= objective function) given x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Write the Jacobian function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jac(x):\n",
    "    # TODO: return vector of partial derivatives\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Run scipy.optimize's minimize function on our QP problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "x0 = np.random.randn(2)\n",
    "opt = {'disp':False}\n",
    "res_cons = minimize(loss, x0, jac=jac, constraints=cons, method='SLSQP', options=opt)\n",
    "\n",
    "print('\\nConstrained:')\n",
    "print(res_cons)\n",
    "\n",
    "x1, x2 = res_cons['x']\n",
    "f = res_cons['fun']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will plot the solution in 3D axes. Feel free to change the code to adjust the view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "# plotting\n",
    "xgrid = np.mgrid[-2:4:0.1, 1.5:5.5:0.1]\n",
    "xvec = xgrid.reshape(2, -1).T\n",
    "F = np.vstack([loss(xi) for xi in xvec]).reshape(xgrid.shape[1:])\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.hold(True)\n",
    "ax.plot_surface(xgrid[0], xgrid[1], F, rstride=1, cstride=1,\n",
    "                cmap=plt.cm.jet, shade=True, alpha=0.9, linewidth=0)\n",
    "ax.plot3D([x1], [x2], [f], 'og', mec='w', label='Constrained minimum')\n",
    "ax.legend(fancybox=True, numpoints=1)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To go further: Linear programming\n",
    "\n",
    "We begin by defining a linear program, a linear function, $\\mathbf{c}^T\\mathbf{x}$ subject to linear constraints $\\mathbf{A}\\mathbf{x} \\leq \\mathbf{b}$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\text{minimise} & z = -2x_1 - x_2 \\\\\n",
    "\\text{subject to} & x_1 + x_2 \\leq 5 \\\\\n",
    "& 5x_1 + 2x_2 \\leq 10 \\\\\n",
    "& x_1, x_2 \\geq 0\n",
    "\\end{array},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Initialise the matrix $\\mathbf{A}$ and the vectors $\\mathbf{b}$ and $\\mathbf{c}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Initialise A, b, and c as numpy arrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear programs can be solved efficiently with the [Simplex algorithm](https://en.wikipedia.org/wiki/Simplex_algorithm). The program specifies an N-dimensional convex solid, and the algorithm works by moving along the edges of this shape from corner to corner. As any optimal solution must be on the surface, we can discard all points interior to the solid. This reduces the feasible solution space to a finite set. Furthermore, the convexity of the solid guarantees that as long as we repeatedly take upward steps along the surface edges, we will arrive at a maximum in a finite number of steps.\n",
    "\n",
    "![Linear Program](img/lp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Solve the linear program using [scipy's implementation of Simplex](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: solve the linear program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.** Scipy doesn't really support proper QP solvers. When we come to support vector machines, we will implement the solver ourselves."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
